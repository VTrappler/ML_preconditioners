{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2237141e-104e-4644-8fb0-eea3ebd6d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a864e351-6c18-4438-9724-618ca03b2260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2459fed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diago = torch.arange(20).reshape(2, 10)\n",
    "diago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5457f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  3,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  4,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  5,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  6,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  7,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  8,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  9]],\n",
       "\n",
       "        [[10,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0, 11,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0, 12,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0, 13,  0,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0, 14,  0,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0, 15,  0,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0, 16,  0,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0, 17,  0,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0, 18,  0],\n",
       "         [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 19]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag_embed(diago)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5cdb78e7-ce0e-47ad-8361-af898a012e4e",
   "metadata": {},
   "source": [
    "## Circular CNN 1D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "adf844e4-0a27-4952-aea2-0c8e1faf4974",
   "metadata": {},
   "source": [
    "Set easier weights and input vector for easy visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970280a1-4c05-4d7e-bea6-8c00e5724b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]]]) torch.Size([1, 1, 10])\n",
      "tensor([[[25., 30., 25., 30., 25., 30., 25., 30., 25., 30.]]],\n",
      "       grad_fn=<ConvolutionBackward0>) torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "n_in = 10\n",
    "kernel_size = 5\n",
    "dilation = 2\n",
    "padding_mode = \"circular\"\n",
    "conv_layer_simple = torch.nn.Conv1d(\n",
    "    1,\n",
    "    1,\n",
    "    kernel_size,\n",
    "    padding=(kernel_size // 2) * dilation,\n",
    "    dilation=dilation,\n",
    "    padding_mode=padding_mode,\n",
    "    bias=False,\n",
    ")\n",
    "# in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None\n",
    "conv_layer_simple.weight = torch.nn.Parameter(torch.ones_like(conv_layer_simple.weight))\n",
    "x = torch.rand(size=(1, 1, 10))\n",
    "for i in range(n_in):\n",
    "    x[:, :, i] = 1.0 * i + 1\n",
    "print(x, x.shape)\n",
    "print(conv_layer_simple(x), conv_layer_simple(x).shape)\n",
    "\n",
    "\n",
    "# conv_layer_simple = torch.nn.Conv1d(1, 1, kernel_size, padding=kernel_size//2, dilation=3, padding_mode=padding_mode, bias=False)\n",
    "# # in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None\n",
    "# # conv_layer_simple.weight = torch.nn.Parameter(torch.ones_like(conv_layer_simple.weight))\n",
    "# print(x)\n",
    "# print(conv_layer_simple(x))\n",
    "conv_layers_dilations = []\n",
    "for dilation in [1, 2, 4]:\n",
    "    conv_layers_dilations.append(\n",
    "        torch.nn.Conv1d(\n",
    "            1,\n",
    "            1,\n",
    "            kernel_size,\n",
    "            padding=(kernel_size // 2) * dilation,\n",
    "            dilation=dilation,\n",
    "            padding_mode=padding_mode,\n",
    "            bias=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "class ParallelConv1DDilations(torch.nn.Module):\n",
    "    def __init__(self, n_in, dilation_list, kernel_size):\n",
    "        self.n_in = n_in\n",
    "        self.dilation_list = dilation_list\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_layers_dilations = []\n",
    "        for dilation in self.dilation_list:\n",
    "            self.conv_layers_dilations.append(\n",
    "                torch.nn.Conv1d(\n",
    "                    1,\n",
    "                    1,\n",
    "                    self.kernel_size,\n",
    "                    padding=(self.kernel_size // 2) * dilation,\n",
    "                    dilation=dilation,\n",
    "                    padding_mode=padding_mode,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.concatenate(\n",
    "            [conv_layer.forward(x) for conv_layer in self.conv_layers_dilations], dim=1\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "542ed602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4299,  0.3688,  0.5398,  0.7141,  0.6581,  0.8938,  0.7079,\n",
       "           0.7749,  0.8156,  0.5879],\n",
       "         [-0.0438, -0.2657, -0.0893, -0.1023, -0.4766, -0.1567, -0.2975,\n",
       "          -0.3517, -0.4367, -0.2881],\n",
       "         [-0.1427, -0.3179, -0.3673, -0.2116, -0.4528, -0.3101, -0.2970,\n",
       "          -0.3614, -0.4629, -0.2918]],\n",
       "\n",
       "        [[ 0.4660,  0.6333,  0.8774,  0.8845,  0.7050,  0.5118,  0.8819,\n",
       "           0.8486,  0.7146,  0.4169],\n",
       "         [-0.0149, -0.1157, -0.5046, -0.3921, -0.0764, -0.2854, -0.3283,\n",
       "          -0.4495, -0.2502, -0.2651],\n",
       "         [-0.2370, -0.3376, -0.4923, -0.4693, -0.0973, -0.3303, -0.4750,\n",
       "          -0.5025, -0.2038, -0.2931]]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parconv = ParallelConv1DDilations(10, [1, 2, 4], kernel_size=3)\n",
    "x = torch.rand((2, 1, 10))\n",
    "parconv.forward(x)\n",
    "# parconv.conv_layers_dilations[0](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f022c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_vec = torch.nn.Conv1d(1, 5, kernel_size, padding=kernel_size//2, padding_mode=padding_mode, bias=True)\n",
    "layers_vec = torch.nn.Sequential(conv_layer_vec, torch.nn.Conv1d(5, 5, kernel_size, padding=kernel_size//2, padding_mode=padding_mode, bias=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49783021",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer_val = torch.nn.Conv1d(1, 1, kernel_size, padding=kernel_size//2, padding_mode=padding_mode, bias=True)\n",
    "layers_val = torch.nn.Sequential(conv_layer_val, torch.nn.Flatten(), torch.nn.Linear(n_in, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "071e3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_conv1D(n_in, n_c, kernel_size):\n",
    "    padding_mode = 'circular'\n",
    "\n",
    "    layers_vec = torch.nn.Sequential(\n",
    "        torch.nn.Conv1d(\n",
    "            1, n_c, kernel_size, padding=kernel_size // 2, padding_mode=padding_mode\n",
    "        ),\n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Conv1d(\n",
    "            n_c, n_c, kernel_size, padding=kernel_size // 2, padding_mode=padding_mode\n",
    "        ),        \n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Conv1d(\n",
    "            n_c, n_c, kernel_size, padding=kernel_size // 2, padding_mode=padding_mode\n",
    "        )\n",
    "    )\n",
    "    return layers_vec\n",
    "\n",
    "def construct_conv1D_singularvalue(n_in, n_c, kernel_size):\n",
    "    padding_mode = 'circular'\n",
    "    layers_vec = torch.nn.Sequential(\n",
    "        torch.nn.Conv1d(\n",
    "            1, n_c, kernel_size, padding=kernel_size // 2, padding_mode=padding_mode\n",
    "        ),\n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Conv1d(\n",
    "            n_c, n_c, kernel_size, padding=kernel_size // 2, padding_mode=padding_mode\n",
    "        ),\n",
    "        torch.nn.AdaptiveAvgPool1d(1)\n",
    ")\n",
    "    return layers_vec\n",
    "\n",
    "class ConvLayersSVD(torch.nn.Module):\n",
    "    def __init__(self, state_dimension, n_latent, kernel_size) -> None:\n",
    "        super().__init__()\n",
    "        self.state_dimension = state_dimension\n",
    "        self.n_latent = n_latent\n",
    "        self.kernel_size = kernel_size\n",
    "        self.layers_vec = construct_conv1D(self.state_dimension, self.n_latent, kernel_size=self.kernel_size)\n",
    "        self.layers_sing = construct_conv1D_singularvalue(self.state_dimension, self.n_latent, kernel_size=self.kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.atleast_2d(x)\n",
    "        x = x.view(len(x), 1, -1)\n",
    "        n_batch = len(x)\n",
    "        vectors = torch.nn.functional.normalize(self.layers_vec(x), dim=-1)\n",
    "        # print(vectors.shape)\n",
    "        singvals = self.layers_sing(x)\n",
    "        # print(singvals.shape)\n",
    "        return torch.concat((vectors, singvals.view(n_batch, self.n_latent, 1)), -1).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82861b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e6db008-9185-4088-9d9c-fd9f7ed2df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvlay = ConvLayersSVD(n_in, 5, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e2b412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2942,  0.3274,  0.2211,  0.3080, -0.2811],\n",
       "         [ 0.2907,  0.2810,  0.5182,  0.2999, -0.3633],\n",
       "         [ 0.3293,  0.2503,  0.4435,  0.3172, -0.3614],\n",
       "         [ 0.2699,  0.3984,  0.0969,  0.3180, -0.3797],\n",
       "         [ 0.3113,  0.2916,  0.4578,  0.3108, -0.3297],\n",
       "         [ 0.3428,  0.3134,  0.2535,  0.3288, -0.3240],\n",
       "         [ 0.3065,  0.3335,  0.1450,  0.3033, -0.2886],\n",
       "         [ 0.3183,  0.3428,  0.3394,  0.3114, -0.3000],\n",
       "         [ 0.3337,  0.3553,  0.1707,  0.3408, -0.3017],\n",
       "         [ 0.3559,  0.2331,  0.1930,  0.3220, -0.1903],\n",
       "         [ 0.0844, -0.0524, -0.0174,  0.1394, -0.2755]],\n",
       "\n",
       "        [[ 0.2940,  0.2606,  0.5170,  0.3006, -0.3389],\n",
       "         [ 0.2797,  0.3810,  0.2685,  0.3050, -0.4163],\n",
       "         [ 0.2997,  0.3844,  0.2684,  0.3315, -0.3981],\n",
       "         [ 0.3725,  0.2119,  0.2318,  0.3381, -0.2341],\n",
       "         [ 0.3020,  0.3187,  0.1641,  0.3029, -0.2447],\n",
       "         [ 0.2794,  0.2935,  0.5023,  0.2828, -0.3293],\n",
       "         [ 0.3142,  0.3839,  0.3535,  0.3263, -0.3770],\n",
       "         [ 0.3518,  0.2975,  0.0437,  0.3450, -0.2660],\n",
       "         [ 0.3394,  0.3679,  0.1265,  0.3236, -0.2480],\n",
       "         [ 0.3159,  0.1879,  0.3358,  0.3006, -0.2361],\n",
       "         [ 0.0860, -0.0567, -0.0157,  0.1467, -0.2785]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvlay(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a54c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class ParallelConv1DDilations(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, n_in, dilation_list: List = [1, 2, 4], kernel_size: int = 3, skip=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_in = n_in\n",
    "        self.padding_mode = \"circular\"\n",
    "        self.dilation_list = dilation_list\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_layers_dilations = []\n",
    "        self.skip_connection = skip\n",
    "        for dilation in self.dilation_list:\n",
    "            self.conv_layers_dilations.append(\n",
    "                torch.nn.Conv1d(\n",
    "                    1,\n",
    "                    1,\n",
    "                    self.kernel_size,\n",
    "                    padding=(self.kernel_size // 2) * dilation,\n",
    "                    dilation=dilation,\n",
    "                    padding_mode=self.padding_mode,\n",
    "                    bias=False,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        forw = [conv_layer.forward(x) for conv_layer in self.conv_layers_dilations]\n",
    "        forw.append(x)\n",
    "        return torch.concatenate(forw, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96159605",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Invalid padding size, expected 0 but got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m conv \u001b[39m=\u001b[39m ParallelConv1DDilations(\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m conv(torch\u001b[39m.\u001b[39;49marange(\u001b[39m10\u001b[39;49m)\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39m10\u001b[39;49m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m, in \u001b[0;36mParallelConv1DDilations.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m     forw \u001b[39m=\u001b[39m [conv_layer\u001b[39m.\u001b[39mforward(x) \u001b[39mfor\u001b[39;00m conv_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layers_dilations]\n\u001b[1;32m     28\u001b[0m     forw\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mconcatenate(forw, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 27\u001b[0m     forw \u001b[39m=\u001b[39m [conv_layer\u001b[39m.\u001b[39;49mforward(x) \u001b[39mfor\u001b[39;00m conv_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_layers_dilations]\n\u001b[1;32m     28\u001b[0m     forw\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     29\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mconcatenate(forw, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:313\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_conv_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(F\u001b[39m.\u001b[39;49mpad(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode),\n\u001b[1;32m    307\u001b[0m                         weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    308\u001b[0m                         _single(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv1d(\u001b[39minput\u001b[39m, weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    310\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Invalid padding size, expected 0 but got 2"
     ]
    }
   ],
   "source": [
    "conv = ParallelConv1DDilations(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff68ac-b02a-49f8-9860-4271d4e5920f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([3, 1, 10]), y.shape=torch.Size([3, 10, 5])\n",
      "x.shape=torch.Size([3, 1, 10]), layers_vec(x).shape=torch.Size([3, 5, 10])\n",
      "x.shape=torch.Size([3, 1, 10]), layers_val(x).shape=torch.Size([3, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 5, 10]), torch.Size([3, 5]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"{x.shape=}, {y.shape=}\")\n",
    "print(f\"{x.shape=}, {layers_vec(x).shape=}\")\n",
    "print(f\"{x.shape=}, {layers_val(x).shape=}\")\n",
    "y, z = layers_vec(x), layers_val(x)\n",
    "y.shape, z.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6086e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 11, 5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcbbe15-0aa3-466b-8f53-8e8927cb083c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3c8ae6c9d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv_matrix = torch.zeros((n_in, n_in))\n",
    "for i in range(n_in):\n",
    "    conv_matrix[i, i % n_in] = 1.0\n",
    "    for j in range(kernel_size):\n",
    "        conv_matrix[i, (i+j - kernel_size//2) % n_in] = 1.0\n",
    "plt.imshow(conv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106066d-fee8-4564-8193-a08392ad3655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(conv_matrix @ x.squeeze() == y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff85d54d-6c7f-446b-a2fc-50bf182c1bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 2., 3., 4., 5.]]])\n",
      "tensor([[[ 6.,  3.,  6.,  9., 12.,  9.]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "305980e6-ec30-4708-81a7-79949f6f4a1e",
   "metadata": {},
   "source": [
    "## Self Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47308a-6da8-4f5c-af1a-ca5bdd7b716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d917e8-b6c1-418b-85a2-65061317c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = torch.nn.MultiheadAttention(embed_dim=embed_dim, num_heads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527d32f-b5b2-4e25-9631-d4765ecd7cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
